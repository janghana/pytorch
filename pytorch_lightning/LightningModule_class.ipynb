{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#6-1. LightningModule Class\n",
        "<hr>"
      ],
      "metadata": {
        "id": "Y6koZPjk_1Bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch lightning에서는 trainer와 모델이 상호작용을 할 수 있도록 pytorch의 nn.Module의 상위 클래스인 lightning module을 구현해야 합니다. 기존 PyTorch는 DataLoader, Mode, optimizer, Training loof 등을 전부 따로따로 코드로 구현을 해야하는데 Pytorch Lightning에서는 Lightning Model class 안에 이 모든것을 한번에 구현하도록 되어있습니다.\n",
        "\n",
        "ligthning module을 정의하기 위해 LightningModule 클래스를 상속받고 모델, training, validation, test 루프 그리고 optimizer 등을 구현해야 합니다.\n",
        "\n",
        "Training 루프는 training_step 메소드에 있고, validation 루프는 validation_step 메소드에 들어가있습니다. 메트릭의 일반적인 리포팅은 validation_epoch_end 메소드에 있습니다. Model 클래스 안에는, training_step과 validation_step 모두 배치(batch)에서 x와 y를 가져오기 위해 step 메소드를 호출합니다. 또한 forward pass와 loss를 리턴하기 위해 foward를 호출합니다. Training이 끝나면, 우리의 validation 루프가 호출되고 epoch이 끝날 때 validation_epoch_end가 호출되서 결과가 누적되고 score가 계산됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "fQCeKK0i_6Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        pass\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        pass\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        pass\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "imN9koD3_2rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lightning Module은 6가지로 구성됩니다.\n",
        "\n",
        "* Computations (init).\n",
        "* Train loop (training_step)\n",
        "* Validation loop (validation_step)\n",
        "* Test loop (test_step)\n",
        "* Prediction loop (predict_step)\n",
        "* Optimizers (configure_optimizers)\n",
        "\n",
        "\n",
        "## init\n",
        "init 는 초기화 메서드 입니다. Lightning Module class에서 사용할 신경망을 정의 합니다. Pytorch에 신경망 Layer를 생성하려면, torch.nn.module에서 불러오거나 확장해야 합니다.\n",
        "\n",
        "## forward\n",
        "forward은 Pytorch에서처럼 추론에 사용됩니다. forward는 모델의 추론 결과를 제공하고 싶을 때 사용합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ewQcfN8WBW-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "  return self.model(x)"
      ],
      "metadata": {
        "id": "PQhNxVbiBeSa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training_step\n",
        "training_step 은 nn.Module의 forward와 유사하지만, 단일 배치에서의 손실을 반환해야 하며, 이는 train loop로 자동 반복됩니다. training_step은 학습 루프의 body 부분을 나타냅니다. 이 메소드에서는 argument로 training 데이터로더가 제공하는 batch와 해당 batch의 인덱스가 주어지고 학습 로스를 계산하여 리턴합니다. pytorch lightning은 편리하게도 batch의 텐서를 cpu 혹은 gpu 텐서로 변경하는 코드를 따로 추가하지 않아도 trainer의 설정에 따라 자동으로 적절한 타입으로 변경해줍니다.\n",
        "\n",
        "만약 epoch-level metric을 계산하고 log를 하려면 .log 메서드를 사용합니다. 만약에 각 training_step의 결과로 무엇인가 할 일이 있으면 training_epoch_end 메서드에 작성합니다.\n",
        "\n",
        "log()개체가 자동으로 complete epoch와 device에서 요청된 metrics(통계)를 줄일 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "40UmW3hUCApX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    y_hat = self.model(x)\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    # logs metrics for each training_step,\n",
        "    # and the average across the epoch, to the progress bar and logger\n",
        "    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "def training_epoch_end(self, training_step_outputs):\n",
        "    for pred in training_step_outputs:\n",
        "        ..."
      ],
      "metadata": {
        "id": "kLrJQyokB2nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validation_step\n",
        "loss 및 metric logging을 위한 validation_step및 test_step을 추가할 수 있습니다. validation_step은 학습 중간에 모델의 성능을 체크하는 용도로 사용합니다. training_step과 마찬가지로 validation 데이터로더에서 제공하는 배치를 가지고 확인하고자 하는 통계량을 기록할 수 있습니다.\n",
        "\n",
        "만약에 각 validation_step의 결과로 무엇인가 할 일이 있으면 validation_epoch_end 메서드에 작성합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "idYMtzftDwC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    y_hat = self.model(x)\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    self.log(\"val_loss\", loss)\n",
        "    pred = ...\n",
        "    return pred\n",
        "\n",
        "def validation_epoch_end(self, validation_step_outputs):\n",
        "    for pred in validation_step_outputs:\n",
        "        ...\n"
      ],
      "metadata": {
        "id": "JDuIIXbGD0cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test_step\n",
        "test_step은 앞의 두 함수와 비슷하게 test 데이터로더에서 제공하는 배치를 가지고 확인하고 싶은 통계량을 기록하는데 사용할 수 있습니다. test loop 코드는 validation loop 코드와 거의 동일합니다. 호출할 때는 test_step()메서드 를 재정의해야 합니다. 테스트 루프는 test()가 사용될 때만 호출된다는 것 입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "8l5v852dEDlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call after training\n",
        "trainer = Trainer()\n",
        "trainer.fit(model)\n",
        "\n",
        "# automatically auto-loads the best weights from the previous run\n",
        "trainer.test(dataloaders=test_dataloader)"
      ],
      "metadata": {
        "id": "IrELkpuDEKMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## configure_optimizers\n",
        "configure_optimizers에서는 모델의 최적 파라미터를 찾을 때 사용할 optimizer와 scheduler를 구현합니다. Optimizer 와 Learning rate scheduler에 대해서는 다음장에 좀더 자세하게 설명하겠습니다.\n",
        "\n",
        "## Boston House 예제\n",
        "위의 순서에 맞춰 간단한 Boston 집값 예측 예제를 풀어 보겠습니다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HR2s5bwPElDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quFeVlR2Ffn0",
        "outputId": "6f529c94-1978-472d-aba2-12cb2fffe805"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (6.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (2023.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (1.22.4)\n",
            "Collecting lightning-utilities>=0.7.0\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning) (2.0.0+cu118)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.25.2)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch-lightning-2.0.2 torchmetrics-0.11.4 yarl-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch import Tensor, nn\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "\n",
        "#Boston 집값 데이터를 읽어온다.\n",
        "X, y = load_boston(return_X_y=True)\n",
        "\n",
        "class SklearnDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        super().__init__()\n",
        "        scaler = MinMaxScaler() \n",
        "\n",
        "        scaler.fit(X) \n",
        "        self.X = scaler.transform(X)\n",
        "        self.Y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx].astype(np.float32)\n",
        "        y = self.Y[idx].astype(np.float32)\n",
        "        return x, y\n",
        "\n",
        "bostonds = SklearnDataset(X, y)\n",
        "train_loader = DataLoader(bostonds, batch_size=32, shuffle=True, drop_last=True, )\n",
        "\n",
        "class LinRegModel(pl.LightningModule):\n",
        "    def __init__(self, input_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features=13, out_features=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_hat = self.linear(x)\n",
        "        return y_hat\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        # flatten any input\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y_hat = self(x)\n",
        "        loss = F.mse_loss(y_hat, y, reduction=\"sum\")\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "trainer = pl.Trainer()\n",
        "model = LinRegModel(input_dim=13)\n",
        "trainer.fit(model, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "n_UpxQFLEl40",
        "outputId": "009c8618-a954-4d5a-d2a4-0899bd0cf434"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fed92dd61393>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_boston' from 'sklearn' (/usr/local/lib/python3.9/dist-packages/sklearn/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_Oj6n8OImOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Y2tN0ChFYYj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}