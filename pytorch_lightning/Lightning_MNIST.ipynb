{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#6-2. Lightning 예제 (MNIST)\n"
      ],
      "metadata": {
        "id": "nwqMWVcQI6--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞의 신경망 모델 설명에서 Fashion MNIST 를 신경망 모델로 풀어 보았다. 이제 동일한 작업을 PyTorch-Lightning 으로 풀어보겠습니다.\n",
        "\n",
        "https://wikidocs.net/edit/page/156984 https://wikidocs.net/edit/page/156984\n",
        "\n",
        "FashionMNIST 데이터를 가져와서 DataLoader 를 만드는 코드는 앞장의 내용과 동일 합니다. 다른 점은 train 데이터셋의 일부를 random으로 샘플링해 validation 용도로 사용하겠습니다.\n",
        "\n",
        "ligthning module을 정의하고 training, validation, test 그리고 optimizer 등을 구현해야 합니다.\n",
        "\n",
        "init 에서는 신경망 모델을 구성해 줍니다. 앞장의 예제와 동일하게 Linear, BatchNorm1d, ReLU 순서로 Layer를 쌓아줍니다. 각각의 Layer에 대한 설명은 앞장을 참고 하세요.\n",
        "\n"
      ],
      "metadata": {
        "id": "Le_sNyl9Jn8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsrxTQ57K0wm",
        "outputId": "31796544-3c99-486d-ebfa-4f9cfeb45cd7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (2023.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (1.22.4)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (23.1)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities>=0.7.0\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.27.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (16.0.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.11.0->pytorch_lightning) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.11.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch_lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch_lightning-2.0.2 torchmetrics-0.11.4 yarl-1.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiDnfwVwNsxN",
        "outputId": "0576169c-4933-4d03-c321-a3bdb6091273"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.core import LightningModule\n",
        "from torchmetrics import functional as FM\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "LQeHH0E9G8b2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def __init__(self):\n",
        "  super().__init__()\n",
        "  self.model = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28*28, 512),\n",
        "      torch.nn.BatchNorm1d(512),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(512, 256),\n",
        "      torch.nn.BatchNorm1d(256),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(256, 64),\n",
        "      torch.nn.BatchNorm1d(64),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(64, 10)\n",
        "  )"
      ],
      "metadata": {
        "id": "xz0Gq5PrI7aC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "forward는 모델의 예측 결과를 제공하고 싶을 때 사용합니다. argument로 받은 x를 init에서 정의한 모델로 전달하여 그 결과를 리턴합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "wWaor1H_AEH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "  return self.model(x)"
      ],
      "metadata": {
        "id": "21mbxGBEJuDe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training_step은 학습 루프의 body 부분을 나타냅니다. 이 메소드에서는 argument로 training 데이터로더가 제공하는 batch와 해당 batch의 인덱스가 전달 됩니다. training_step에서는 손실함수를 사용하여 loss를 계산하고 리턴하면 됩니다. 모델의 output과 정답 라벨 사이의 cross entropy loss를 구해서 넘겨줍니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "N9t5jAPGARXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "ZpW9Yvt9AF0k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "validation_step은 학습 중간에 모델의 성능을 체크하는 용도로 사용합니다. argument로 training 데이터로더가 제공하는 batch와 해당 batch의 인덱스가 전달 됩니다. accuracy 함수는 pytorch_lightning.metrics.functional에서 정의되어 있는 함수로 예측한 y_pred값이 실제 y값과 일치하는 비율을 구해줍니다. 하나의 값을 표시할 때는 self.log(<변수 이름=\"\">, <값>)과 같이 할 수 있고 여러 개의 변수를 표시하고 싶으면 아래와 같이 self.log_dict로 변수 이름, 값 쌍을 가지고 있는 딕셔너리를 표시할 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "E62YXwELDNdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        pred_y = self(x)\n",
        "        acc = FM.accuracy(pred_y, y)\n",
        "        loss = F.cross_entropy(pred_y, y)\n",
        "        metrics = {'val_acc': acc, 'val_loss': loss}\n",
        "        self.log_dict(metrics)\n"
      ],
      "metadata": {
        "id": "DJaVVWC6CNFO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_step은 앞의 test DataLoader에서 제공하는 batch 데이터를 가지고 확인하고 싶은 통계량을 기록하는데 사용할 수 있습니다. 제공되는 batch 데이터만 다를 뿐 코드는 validation_step과 동일 합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "f2sb3G29D_gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        pred_y = self(x)\n",
        "        acc = FM.accuracy(pred_y, y)\n",
        "        loss = F.cross_entropy(pred_y, y)\n",
        "        metrics = {'val_acc': acc, 'val_loss': loss}\n",
        "        self.log_dict(metrics)"
      ],
      "metadata": {
        "id": "3v5HiAYbDRsT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "configure_optimizers에서는 모델의 optimizer와 scheduler를 구현합니다. 앞장과 동일하도록 Adam optimzer를 사용하도록 하겠습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "TQivgbIfEw-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_optimizers(self):\n",
        "        optimizer = Adam(self.parameters())\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "T-_fDMDWEKHY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 학습을 하고 테스트를 하면 됩니다. ytorch lightning에서 학습을 위해 추가로 작성해야 할 코드는 매우 짧습니다.\n",
        "\n",
        "모델을 학습하기 위해서는 학습 로직을 정하는 Trainer를 생성해야 합니다. gpus가 0일 때는 cpu를 사용하고 gpus가 1 이상이면 gpu를 사용하여 모델을 학습합니다. gpus가 2 이상이면 자동으로 다중 gpu를 활용해 분산 학습을 진행하게 됩니다.\n",
        "\n",
        "Trainer와 lightning module을 정의하고 난 뒤에 Trainer의 fit 함수로 모델을 학습할 수 있습니다. fit의 파라미터로 모델, training 데이터로더와 validation 데이터로더를 넘겨줍니다. test 함수에 test 데이터로더를 넘겨주어 모델을 테스트할 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "24KkHVXlF1lL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음과 같은 학습 결과를 얻을 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "5htUDuYdG47G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LitModel()\n",
        "trainer = Trainer(max_epochs=epochs, gpus=1)\n",
        "trainer.fit(model, train_dataloader, val_dataloader)\n",
        "trainer.test(test_dataloaders=test_dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "iMzUiKn6Fp3i",
        "outputId": "cd041ce6-1864-4b2e-b517-3bf95c3eaace"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ddce2673541e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLitModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LitModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import ToTensor\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "import pytorch_lightning.metrics.functional as FM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터셋 다운로드 및 전처리\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "# 훈련 데이터셋을 훈련 및 검증 데이터셋으로 분리\n",
        "train_dataset, val_dataset = random_split(training_data, [55000, 5000])\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# 모델 정의\n",
        "class LitModel(LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 512),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            torch.nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 64),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    # 순전파 함수\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    # 훈련 스텝\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    # 검증 스텝\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        acc = FM.accuracy(logits, y)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        metrics = {'val_acc': acc, 'val_loss': loss}\n",
        "        self.log_dict(metrics)\n",
        "\n",
        "    # 테스트 스텝\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        acc = FM.accuracy(logits, y)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        metrics = {'test_acc': acc, 'test_loss': loss}\n",
        "        self.log_dict(metrics)    \n",
        "\n",
        "    # 옵티마이저 설정\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "# 모델 생성 및 훈련\n",
        "model = LitModel()\n",
        "trainer = Trainer(max_epochs=epochs, gpus=0)\n",
        "trainer.fit(model, train_dataloader, val_dataloader)\n",
        "\n",
        "# 모델 테스트\n",
        "trainer.test(test_dataloaders=test_dataloader)\n",
        "\n",
        "# 레이블 및 클래스 목록\n",
        "label_tags = {\n",
        "    0: 'T-Shirt', \n",
        "    1: 'Trouser', \n",
        "    2: 'Pullover', \n",
        "    3: 'Dress', \n",
        "    4: 'Coat', \n",
        "    5: 'Sandal', \n",
        "    6: 'Shirt',\n",
        "    7: 'Sneaker', \n",
        "    8: 'Bag', \n",
        "    9: 'Ankle Boot'\n",
        "}\n",
        "\n",
        "# 예측 결과를 시각화하기 위한 설정\n",
        "columns = 6\n",
        "rows = 6\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "\n",
        "# 모델 평가 모드\n",
        "model.eval()\n",
        "\n",
        "# 예측 결과를 그리드 형태로 시각화\n",
        "for i in range(1, columns*rows+1):\n",
        "    data_idx = torch.randint(len(test_dataloader), (1,)).item()\n",
        "    input_img = test_data[data_idx][0].unsqueeze(dim=0)\n",
        "\n",
        "    output = model(input_img)\n",
        "    _, argmax = torch.max(output, 1)\n",
        "    pred = label_tags[argmax.item()]\n",
        "    label = label_tags[test_data[data_idx][1]]\n",
        "\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    if pred == label:\n",
        "        plt.title(pred)\n",
        "        cmap = 'Blues'\n",
        "    else:\n",
        "        plt.title(pred + '=>' +  label)\n",
        "        cmap = 'Reds'\n",
        "    plot_img = test_data[data_idx][0][0,:,:]\n",
        "    plt.imshow(plot_img, cmap=cmap)\n",
        "    plt.axis('off')\n",
        "\n",
        "# 그림 표시\n",
        "plt.show() \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "-Y3YfIX3GRvU",
        "outputId": "4fc82d0e-adc5-48f6-c1f3-de2963ad1a7e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-39e5d993be5f>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# 모델 생성 및 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLitModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/utilities/argparse.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'gpus'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bHlJhNV8GXaB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}