{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#5-1. Transform\n",
        "\n",
        "Pytorch는 이미지 분류, segmentation, 텍스트 처리, object Identification과 같은 다양한 작업에 광범위하게 사용되는 딥 러닝 프레임워크입니다. 이러한 경우 다양한 유형의 데이터를 처리해야 합니다. 그리고 대부분의 경우 데이터가 데이터가 항상 머신러닝 알고리즘 학습에 필요한 최종 처리가 된 형태로 제공되지는 않습니다. transform 을 해서 데이터를 조작하고 학습에 적합하게 만들어야 합니다.\n",
        "\n",
        "PyTorch의 torchvision 라이브러리는 transforms 에서 다양한 변환 기능을 제공합니다. transform을 사용하여 데이터의 일부 조작을 수행하고 훈련에 적합하게 만들 수 있습니다.\n",
        "\n",
        "* transforms.ToTensor() - 데이터를 tensor로 바꿔준다.\n",
        "* transforms.Normalize(mean, std, inplace=False) - 정규화한다.\n",
        "* transforms.ToPILImage() - csv 파일로 데이터셋을 받을 경우, PIL image로 바꿔준다.\n",
        "* transforms.Compose - 여러 단계로 변환해야 하는 경우, Compose를 통해 여러 단계를 묶을 수 있다.\n",
        "\n",
        "Dataset 클래스의 __getitem__ 함수내에서 데이터를 변환하여 리턴될 때 주로 사용됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ALlZwynnHQoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchvisionMaskDataset(Dataset):\n",
        "    def __init__(self, path, transform=None):\n",
        "        self.path = path\n",
        "        self.imgs = list(sorted(os.listdir(self.path)))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_image = self.imgs[idx]\n",
        "        file_label = self.imgs[idx][:-3] + 'xml'\n",
        "        img_path = os.path.join(self.path, file_image)\n",
        "        ....\n",
        "        ....\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "torchvision_transform = transforms.Compose([\n",
        "    transforms.Resize((300, 300)), \n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomHorizontalFlip(p = 1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "torchvision_dataset = TorchvisionMaskDataset(\n",
        "    path = 'images/',\n",
        "    transform = torchvision_transform\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "NudHL_riHR_U",
        "outputId": "661f1aeb-8015-4dae-fd21-1ab9d3c150ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0ae3ee7e7f57>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    ....\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iu0sFK79Hmgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ToTensor\n",
        "ToTensor는 매우 일반적으로 사용되는 conversion transform입니다. PyTorch에서 우리는 주로 텐서 형태의 데이터로 작업합니다. 입력 데이터가 NumPy 배열 또는 PIL 이미지 형식인 경우 ToTensor를 사용하여 텐서 형식으로 변환할 수 있습니다. transforms.ToTensor()\n",
        "\n",
        "2. Normalize\n",
        "Normalize 작업은 텐서를 가져와 평균 및 표준 편차로 정규화합니다. transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) - mean : (sequence)형식으로 평균을 입력하며, 괄호 안에 들어가 있는 수의 개수가 채널의 수이다. - std : (sequence)형식으로 표준을 입력하며, 마찬가지로 괄호 안에 들어가 있는 수의 개수가 채널의 수이다.\n",
        "\n",
        "3. CenterCrop\n",
        "이것은 중앙에서 주어진 텐서 이미지를 자릅니다. transform.CenterCrop(height, width) 형식으로 자르려는 크기를 입력으로 제공할 수 있습니다.\n",
        "\n",
        "4. RandomHorizontalFlip\n",
        "이 변환은 주어진 확률로 이미지를 무작위로 수평으로 뒤집을(flip) 것입니다. 이 확률은 매개변수 'p'를 통해 설정할 수 있습니다. p의 기본값은 0.5입니다.\n",
        "\n",
        "5. RandomRotation\n",
        "이 변환은 이미지를 각도만큼 무작위로 회전합니다. 도(degree) 단위의 각도는 해당 매개변수 \"degree\"에 대한 입력으로 제공될 수 있습니다.\n",
        "\n",
        "6. Grayscale\n",
        "이 변환은 원본 RGB 이미지를 회색조로 변경합니다. \" num_output_channels\" 매개변수에 입력으로 원하는 채널 수를 제공할 수 있습니다 .\n",
        "\n",
        "7. 가우시안 블러\n",
        "여기에서 이미지는 무작위로 선택된 가우시안 흐림 효과로 흐려집니다. kernel_size 인수를 제공하는 것은 필수입니다.\n",
        "\n",
        "8. RandomApply\n",
        "이 변환은 확률로 주어진 transformation 들을 무작위로 적용합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "hr5O93aSHtEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.RandomApply([transforms.RandomSizedCrop(200),transforms.RandomHorizontalFlip()],p=0.6)\n",
        "tensor_img = transform(image)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "blPAskAuHtZa",
        "outputId": "d7522b92-9e93-49fc-cc96-ae49c5961c91"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c1693c3b2456>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomSizedCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtensor_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Compose\n",
        "transform에 여러 단계가 있는 경우, Compose를 통해 여러 단계를 하나로 묶을 수 있습니다. transforms에 속한 함수들을 Compose를 통해 묶어서 한번에 처리할 수 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "feNR5VlmHwgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Compose([ \n",
        "   transforms.ToTensor(), \n",
        "   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "MpWOta9eHu21",
        "outputId": "612300e0-fa1a-4f2f-cfe5-c8624ea94130"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a02e33c21b27>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transforms.Compose([ \n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    }
  ]
}