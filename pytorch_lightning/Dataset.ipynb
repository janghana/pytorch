{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Dataset 과 DataLoader\n"
      ],
      "metadata": {
        "id": "X2mRlGv9oaEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 학습시키기 위해서는 Data가 필수적입니다.\n",
        "\n",
        "데이터를 처리하는 머신러닝, 딥러닝코드는 복잡하고 유지보수가 어려울 수 있습니다. 실제 PyTorch로 모델을 만들고 학습을 시키는 프로그램을 작성 할 때 많은 비중을 차지하는 이슈가 모델에 데이터를 공급해 주는 일입니다.\n",
        "\n",
        "그래서 더 나은 가독성(readability)과 모듈성(modularity)을 위해 데이터셋 코드를 모델 학습 코드로부터 분리하는 방법이 필요합니다.\n",
        "\n",
        "딥러닝 단계는 일반적으로 다음 단계로 처리됩니다.\n",
        "\n",
        "* Prepare the data\n",
        "* Build the model\n",
        "* Train the model\n",
        "* Analyze the model's results\n",
        "\n",
        "![https://media.vlpt.us/images/naem1023/post/a659d452-c938-4671-a516-55c059003eec/image.png] (https://media.vlpt.us/images/naem1023/post/a659d452-c938-4671-a516-55c059003eec/image.png) Image : https://media.vlpt.us/images/naem1023/post/a659d452-c938-4671-a516-55c059003eec/image.png\n",
        "\n",
        "딥 러닝 학습 과정에서 가장 중요한 것은 신경망 모델링이 아닙니다. 머신 러닝을 위해 데이터를 준비하는 단계 입니다.\n",
        "\n",
        "프로토타이핑은 개발접근법의 하나로서 개발초기에 시스템의 원형을 간단히 만들어 테스트해보고 부족한 기능의 추가, 변경 및 삭제 등을 반영하여 설계를 다시 하고 프로토타입을 재구축하는 과정을 만족할 때까지 반복해 나가면서 시스템을 개선시켜 나가는 방식을 말합니다\n",
        "\n",
        "딥러닝 프로그래밍은 빠른 프로토타이핑 및 maintaining usability를 유지하기 위해 가능한 한 모듈식으로 프로그래밍 해야 합니다. 그래서 Dataset 처리는 모든 딥러닝 학습에서 매우 중요한 부분이며 모델링과 별도로 유지해야 합니다.\n",
        "\n",
        "PyTorch는 데이터를 불러오는 과정을 쉽게해주고, 또 잘 사용한다면 코드의 가독성을 보다 높여줄 수 있는 도구들을 제공합니다.\n",
        "\n",
        "PyTorch는 데이터 처리를 위한 두 가지 클래스( torch.utils.data.Dataset 및 torch.utils.data.DataLoader)를 제공 합니다.\n",
        "\n",
        "PyTorch는 torch.utils.data.DataLoader 와 torch.utils.data.Dataset 의 두 가지 데이터 기본 요소를 제공하여 미리 준비해된(pre-loaded) 데이터셋 뿐만 아니라 가지고 있는 데이터를 사용할 수 있도록 합니다.\n",
        "\n",
        "다음 링크를 가보시면 PyTorch의 TORCHVISION.DATASETS 에서 기본으로 제공하는 데이타셋을 확인 할 수 있습니다. (https://pytorch.org/vision/stable/datasets.html)\n",
        "\n",
        "Dataset 은 샘플과 정답(label)을 저장하고, DataLoader 는 Dataset 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감쌉니다.\n",
        "\n",
        "많은 양의 data를 이용해 딥러닝 모델을 학습시킬 때 data를 한번에 불러오면 시간이 오래걸리고 어떨 때는 메모리가 부족합니다. 데이터를 한번에 다 부르지 않고 하나씩만 불러서 쓰는 방식을 택하면 작은 메모리를 가진 컴퓨터에서도 모델을 돌릴 수 있습니다. 그래서 모든 데이터를 한번에 불러놓고 쓰는 기존의 dataset말고 custom dataset을 만들어야할 필요가 있습니다. 또한 길이가 변하는 input에 대해서 batch를 만들기 위해서는 dataloader에서 batch를 만드는 부분을 수정해야할 필요가 있어 custom dataloader를 사용해야 합니다.\n",
        "\n",
        "사용자 지정 Dataset 과 DataLoader 를 만드는 법을 알아 봅시다.\n",
        "\n"
      ],
      "metadata": {
        "id": "TFKIfbwModLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "머신러닝, 딥러닝 학습에 사용되는 방대한 데이터의 크기 때문에 데이터를 한 번에 불러오기 쉽지 않습니다. 따라서 데이터를 한 번에 부르지 않고 하나씩만 불러서 쓰는 방식을 택해야 합니다. 모든 데이터를 불러놓고 사용하는 기존의 Dataset 말고 Custom Dataset 이 필요합니다.\n",
        "\n",
        "Dataset class는 전체 dataset을 구성하는 단계입니다. input으로는 지도학습에 일반적인 x(input feature)과 y(label)을 tensor로 넣어주면 됩니다. PyTorch의 TensorDataset은 tensor를 감싸는 Dataset입니다.\n",
        "\n",
        "Dataset Class에서 반드시 정의해야 하는 Method 들은 다음과 와 같습니다.\n",
        "\n",
        "* init(self): 여기서 필요한 변수들을 선언한다. init 함수는 Dataset 객체가 생성될 때 한 번만 실행됩니다.\n",
        "* get_item(self, index): 만든 리스트의 index 에 해당하는 샘플을 데이터셋에서 불러오고 전처리를 실행한 다음 tensor 자료형으로 바꾸어 리턴하는 구조이다.\n",
        "* len(self): 학습 데이터의 갯수를 리턴한다.\n"
      ],
      "metadata": {
        "id": "ChbYdFzHBh--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "    # 생성자, 데이터를 전처리 하는 부분   \n",
        "\n",
        "    def __len__(self):\n",
        "    # 데이터셋의 총 길이를 반환하는 부분   \n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "    # idx(인덱스)에 해당하는 입출력 데이터를 반환한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "1xnugI6_oZdU",
        "outputId": "05042d1f-5597-4f75-e5d0-97a204cde2f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d44cc2adfbc7>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    def __len__(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "선형 회귀를 위해 Dataset을 만든다면 다음과 같은 코드가 될것입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "BtHpUViNEyCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "      self.x_data = [[73, 80, 75],\n",
        "                            [93, 99, 93]]\n",
        "      self.y_data = [[152], [185]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      x = torch.FloatTensor(self.x_data[idx])\n",
        "      y = torch.FloatTensor(self.y_data[idx])\n",
        "\n",
        "      return x, y\n",
        "\n",
        "dataset = CustomDataset()\n"
      ],
      "metadata": {
        "id": "EOWeoxdpEs7V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataSet은 DataLoader를 통하여 data를 받아오는 역할을 합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "gI2PZLNzE71Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader\n",
        "DataLoader는 PyTorch 데이터 로딩 유틸리티의 핵심입니다. DataLoader의 가장 중요한 인자는 데이터를 불러올 데이터셋 객체를 나타내는 데이터셋입니다. 모든 Dataset 으로부터 DataLoader 를 생성할 수 있습니다. PyTorch의 DataLoader 는 배치 관리를 담당합니다. DataLoader란 Dataset을 batch기반의 딥러닝모델 학습을 위해서 미니배치 형태로 만들어서 우리가 실제로 학습할 때 이용할 수 있게 형태를 만들어주는 기능을 합니다. DataLoader를 통해 Dataset의 전체 데이터가 batch size로 slice되어 공급됩니다. 앞서 만들었던 dataset을 input으로 넣어주면 여러 옵션(데이터 묶기, 섞기, 알아서 병렬처리)을 통해 batch를 만들어줍니다. DataLoader는 iterator 형식으로 데이터에 접근 하도록 하며 batch_size나 shuffle 유무를 설정할 수 있다.\n",
        "\n",
        "일반적인 사용 방법은 다음과 같다.\n",
        "\n"
      ],
      "metadata": {
        "id": "GWZ31nq-FBTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size = 2,\n",
        "    shuffle = True,\n",
        ")"
      ],
      "metadata": {
        "id": "IL4EYEZGEz2u"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader의 정의는 다음과 같습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "HBjdnj4KFsRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
        "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
        "           pin_memory=False, drop_last=False, timeout=0,\n",
        "           worker_init_fn=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlEoqcPCFZfD",
        "outputId": "5184b78f-adf7-4b00-b8db-b12e7870bbea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f050e22ea90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_size 는 각 minibatch의 크기 즉 한 번의 배치 안에 있는 샘플 사이즈를 말합니다. 통상적으로 2의 제곱수로 설정합니다 (예: 16, 32, 64...) 데이터셋의 크기가 그렇게 크지 않으면 굳이 사용하지 않아도 되지만, 데이터셋의 크기가 매우 큰 경우엔 모든 데이터를 한 번에 넣어서 처리하는 방식을 적용하기엔 무리가 있습니다. 그래서 Mini Batch 라는 개념으로 (묶음) 한 번에 한 묶음씩 처리하는 방식을 사용하게 됩니다. 정리하면, 전체 데이터셋을 batch size 크기로 묶어서 iteration의 수 만큼 실행하는 것입니다.\n",
        "\n",
        "shuffle 은 Epoch 마다 데이터셋을 섞어, 데이터가 학습되는 순서를 바꾸는 기능을 말합니다. 학습할 때는 항상 True로 설정하는 것을 권장합니다.\n",
        "\n",
        "num_worker는 동시에 처리하는 프로세서의 수입니다. 서버에서 돌릴 때는 num_worker를 조절해서 load속도를 올릴 수 있지만, PC에서는 default=0로 설정해야 오류가 안납니다. num_worker 하나를 더 추가 하면 20% 정도 속도가 빨라 진다고 합니다. 그러나 무작정 num_worker 수를 늘린다고 속도가 빨라지는 것은 아닙니다. 공급되는 배치를 처리하는 빠른 프로세서가 있어야 속도가 빨라지므로 적절한 조절이 필요합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3WK7ABAF2WB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##collate_fn\n",
        "DataLoader에는 collate_fn 이라는 파라미터를 지정할 수가 있습니다. 이 파라미터를 사용하면 별도의 데이터 처리 함수를 만들 수 있으며 해당 함수 내의 처리를 데이터가 출력되기 전에 적용됩니다. 기본적으로 default_collate라는 함수는 Dataset이 반환하는 데이터 유형을 확인하고 (x_batch, y_batch) 와 같은 배치로 결합하려고 시도합니다 . 배치단위 데이터의 변환 시에 활용됩니다. 사용자 지정 collate_fn 을 사용하여 사용자 지정 batch 처리를 수행 할 수 있습니다( 예: 다양한 길이의 패딩 시퀀스 또는 사용자 지정 데이터 유형에 대한 지원 추가).\n",
        "\n",
        "사용자 지정 collate_fn 은 데이터 정렬을 사용자 지정으로 처리하는데 사용할 수 있습니다. 모델로 보내기 전, collate_fn 함수는 DataLoader 로부터 생성된 샘플 배치로 동작합니다. collate_fn 의 입력은 DataLoader 에 배치 크기(batch size)가 있는 배치(batch) 데이터이며, collate_fn 은 이를 미리 선언된 데이터 처리 파이프라인에 따라 처리합니다.\n",
        "\n",
        "사용자 정의 collate_fn() 함수는 가변 길이 배치를 채우는 데 자주 사용됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "oCVxzuq7GVEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    word_tensor = torch.tensor([[1.], [0.], [45.]])\n",
        "    label_tensor = torch.tensor([[1.]])\n",
        "\n",
        "    text_list, classes = [], []\n",
        "    for (_text, _class) in batch:\n",
        "\n",
        "      text_list.append(word_tensor)\n",
        "      classes.append(label_tensor)\n",
        "    text = torch.cat(text_list)\n",
        "    classes = torch.tensor(classes)\n",
        "    return text, classes\n",
        "DL_DS = DataLoader(TD, batch_size=2, collate_fn=collate_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "lYZcAi8qFt5m",
        "outputId": "d3a997d0-ad45-4e56-bb2b-3a848ce6e7ca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d42ed72d243a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mDL_DS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'TD' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수를 활성화하려면 DataLoader 객체를 초기화 할 때 collate_fn = Your_Function_name 매개 변수를 추가하기 만하면 됩니다. collate_fn 함수는 DataLoader 로부터 생성된 샘플 배치로 동작합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "mIlAYF3hG5zD"
      }
    }
  ]
}