{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#6-3. Optimizer"
      ],
      "metadata": {
        "id": "vlXb_Ff6Tuvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델과 데이터가 준비되었으면, 데이터에 매개변수를 최적화하여 모델을 학습하고, 검증하고, 테스트할 차례입니다. 모델을 학습하는 과정은 반복적인 과정을 거칩니다. (에폭(epoch)이라고 부르는) 각 반복 단계에서 모델은 출력을 추측하고, 추측과 정답 사이의 오류(손실(loss))를 계산하고, 매개변수에 대한 오류의 도함수(derivative)를 수집한 뒤, 최적화 알고리즘을 사용하여 이 파라매터들을 최적화(optimize)합니다. 즉 딥러닝에서 모델을 학습시킨다는건 최적화(optimization) 태스크를 수행하는 것과 같습니다. 최적화란, 좌측 그래프처럼 정의된 손실 함수(loss funciton)의 최솟값을 찾아나가는 일련의 과정을 말합니다.\n",
        "\n",
        "torch.optim은 다양한 최적화 알고리즘을 구현한 패키지입니다. 최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정입니다. 최적화 알고리즘은 이 과정이 수행되는 방식을 정의합니다. PyTorch에는 SGD, ADAM이나 RMSProp과 같은 많은 종류의 모델과 특화된 데이터에서 더 잘 동작하는 다양한 옵티마이저가 있습니다.\n",
        "\n",
        "예를 들어\n",
        "\n"
      ],
      "metadata": {
        "id": "vLn-sbPmTyZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = optim.Adam([var1, var2], lr=0.0001)"
      ],
      "metadata": {
        "id": "IcIvKWYFTwGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "최적화 단계 수행 모든 옵티마이저는 step() 메소드에서 수행 됩니다 . 두 가지 방법으로 사용할 수 있습니다.\n",
        "\n",
        "optimizer.step() 이 메서드는 대부분의 옵티마이저에서 지원하는 단순화된 버전입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "FBU4Uv3JWKuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input, target in dataset:\n",
        "  optimizer.zero_grad()\n",
        "  output = model(input)\n",
        "  loss = loss_fn(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "S8GLbtxZWLGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Class\n",
        "class torch.optim.Optimizer( 매개변수 , 기본값 ) [source] 모든 옵티마이저의 기본 클래스입니다.\n",
        "\n",
        "사용되는 메서드들은 다음과 같습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "9BHzqY0eWQwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Batch 를 사용하는 GD는 시간이 많이 걸리는 단점이 있습니다. SGD는 Full Batch가 아닌 Mini Batch로 학습을 진행합니다. 가장 기본적인 SGD의 사용 예제입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "7QJJwNyLWWnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "model = MyNetwork()\n",
        "optimizer = torch.optim.SGD(model.parameters() , lr=  0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "7vIwQKxsWRbr",
        "outputId": "c2bad648-329a-465c-dd9b-194fb43d1dee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9ec4de7f0afa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MyNetwork' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 lr은 Learning Rate의 줄임말이며, 얼마만큼 이동시킬 것인가를 결정합니다. lr 의 초기값이 크다면 초반엔 loss값이 빠르게 줄겠지만, 나중에 가면 underfitting이 발생하게 됩니다. 또한, local minima(지역 최소점)과 saddle point(안장점)에 빠질 수 있다는 치명적인 단점이 있습니다.\n",
        "\n",
        "이러한 SGD의 문제를 해결하기 위해 여러가지 개선된 알고리즘이 등장 합니다. optimizer의 step은 정답에 가까워질수록 lr을 줄여야 최적의 해를 구할 수 있는데, SGD의 경우 underfitting이 일어날 떄마다 lr을 줄여주면 더 좋은 결과를 예상 할 수 있습니다. 이런 개념에서 탄생한 것이 AdaGrad입니다. 그러나, step별로 lr를 줄이다 보니 최적의 답에 도달하기 전에 0에 가까워 져서 더 이상 움직이지 않는 문제가 생깁니다. 그것을 방지하기 위해 RMSprop이라는 것이 등장 했습니다. RMSprop은 AdaGrad를 응용한 것으로, 일정한 비율로 step을 조절합니다. Adagrad는 gradient가 0에 수렴하는 문제가 있어서 Rmsprop을 사용합니다.\n",
        "\n",
        "이러한 개선들을 모두 합친 것이 Adam 입니다. Adam은 SGD + Momentum, RMSprop을 같이 사용하여 더욱 효율적으로 gradient를 조절합니다. 보통 Adam을 쓰는 것을 권장합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "9bRGMHdqWkYQ"
      }
    }
  ]
}