{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#6-4. Learning Rate Scheduler"
      ],
      "metadata": {
        "id": "UQOgTiu9ae2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate는 backpropagation 학습 과정에서 모델의 weight인 gradient의 변화/업데이트 보폭(or step-size)을 말합니다. 역전파에서 모델 가중치(weight)는 손실 함수의 오류 추정치를 줄이기 위해 업데이트됩니다. 전체 양을 사용하여 가중치를 변경하는 대신 학습률 값을 곱합니다. 예를 들어, 학습률을 0.5로 설정하는 것은 0.5* 추정 가중치 오류(즉, 가중치에 대한 기울기 또는 전체 오류 변화)로 가중치를 업데이트 하는 것을 의미합니다.\n",
        "\n",
        "##Effect of the learning rate\n",
        "학습률은 옵티마이저가 손실 함수의 최소값에 도달하는 단계의 크기를 제어합니다. 성능에 영향을 주는 요소인 learning rate를 잘못 설정하면 아예 학습이 안되기도 합니다. 그렇기 때문에 learning rate를 어떻게 설정할 지는 매우 중요한 요소입니다. 아래의 그림이 learning rate 설정에 따라 loss 그래프에서 어떻게 최적의 weight를 찾아나가는 지를 보여줍니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "Cd9aHDB_bLj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Source : https://i0.wp.com/neptune.ai/wp-content/uploads/Learning-rate-scheduler.png?resize=1024%2C384&ssl=1"
      ],
      "metadata": {
        "id": "duE7k0-Plb5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "오른쪽 그림처럼 학습률이 크면 알고리즘은 빠르게 학습하지만 알고리즘이 최소값을 중심으로 진동하거나 심지어 뛰어넘을 수도 있습니다. 또한 학습률이 크면 가중치 업데이트가 많아 가중치가 overflow 될 수도 있습니다. 반대로 왼쪽 그림처럼 학습률이 낮으면 가중치 업데이트가 작아서 옵티마이저가 최소값을 향해 천천히 움직입니다. 그래서 옵티마이저는 수렴하는 데 너무 시간이 오래 걸리거나 정체 상태 또는 바람직하지 않은 로컬 최소값에 갇힐 수 있습니다. 가운데 그림은 좋은 학습률 변화의 예로서 알고리즘이 빠르게 수렴할 수 있을 정도로 너무 작지 않고, 알고리즘이 최소값에 도달하지 않고 앞뒤로 점프하지 않을 정도로 너무 크지 않습니다.\n",
        "\n",
        "적절한 학습률을 찾는 이론적인 원리는 간단하지만 너무 크지도 않고 너무 작지도 않는 학습률을 찾는 것은 쉽지 않습니다! 이 문제를 해결하기 위해 learning rate schedule 가 도입되었습니다. 처음부터 끝까지 같은 learning rate를 사용할 수도 있지만, 학습과정에서 learning rate를 조정하는 learning rate scheduler를 사용하면 더 좋은 성능을 기대할 수 있습니다. 처음엔 큰 learning rate(보폭)으로 빠르게 optimize를 하고 최적값에 가까워질수록 learning rate(보폭)를 줄여 미세조정을 하는 것이 학습이 잘된다고 알려져있습니다. learning rate를 decay하는 방법이외에도 learning rate를 줄였다 늘렸다 하는 것이 더 성능향상에 도움이 된다는 연구결과도 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "86bBv-NYldf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rate scheduler\n",
        "Learning rate schedule는 학습이 진행됨에 따라 epoch 또는 iteration 간에 학습률을 조정하는 사전 정의된 프레임워크입니다. Learning rate schedule에 대한 가장 일반적인 두 가지 기술은 다음과 같습니다.\n",
        "\n",
        "* 일정한 Learning rate: 이름에서 알 수 있듯이 학습률을 초기화하고 훈련 중에 변경하지 않습니다.\n",
        "* Learning rate 감소: 초기 학습률을 선택한 다음 스케줄러에 따라 점차적으로 감소시킵니다.\n",
        "\n",
        "훈련 초기에 학습률은 충분히 좋은 가중치에 도달하기 위해 크게 설정됩니다. 시간이 지남에 따라 이러한 가중치는 작은 학습률을 활용하여 더 높은 정확도에 도달하도록 미세 조정됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "vrSi4mI-lzjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step Decay, Cosine Decay\n"
      ],
      "metadata": {
        "id": "OSFyCWl-nokO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "상황마다 조건이 다르기 때문에, 언제 학습률의 증가를 규제(Learning Rate Decay)해야 하는 지를 확실히 아는 것은 쉽지 않을 수 있습니다. Learning Rate가 높은 경우 loss 값을 빠르게 내릴 수는 있지만, 최적의 학습을 벗어나게 만들고, 낮은 경우 최적의 학습을 할 수 잇지만 그 단계까지 너무 오랜 시간이 걸리게 됩니다. 따라서, 처음 시작시 Learning Rate 값을 크게 준 후 일정 epoch 마다 값을 감소시켜서 최적의 학습까지 더 빠르게 도달할 수 있게 하는 방법을 Learning Rate Decay라고 말합니다. Step Decay 는 Learning Rate Decay를 특정 epoch를 기준으로 단계별로 learning rate을 감소시키는 것을 말합니다. 특정 epoch 구간(step) 마다 일정한 비율로 감소 시켜주는 방법을 Step Decay라고 부릅니다. 실제로도 학습률을 조정하는데 있어 가장 널리 사용되는 방법이 이 Step Decay 입니다. 상수값의 비율과 Epoch 단위만으로 학습률을 조정하는 것이기 때문에 사용하기 쉽고, 복잡한 연산의 이해를 요구하지 않으므로 직관적인 방법이기 때문입니다. Step Decay를 사용하면 기존보다 생각해야하는 Hyper Parameter가 늘어나니 세팅해야하는 값들이 증가하고 세팅 값의 경우의 수가 더 많이 생기게 되었습니다. 따라서 당시 사람들은 Hyper Parameter를 조금 덜 사용할 수 있는 다른 방법을 찾게 되었습니다. 그렇게해서 나온 연속적인 방법이 바로 Cosine Decay 입니다. Cosine Decay의 loss 그래프를 보면 Step Decay와는 달리 안정적으로 끊김없이 loss가 감소하는 것을 볼 수 있습니다. Hyper Paremeters를 좀 덜 사용하고 learning rate도 연속적으로 사용하니 Cosine Decay를 Learning Rate Decay로 좀 더 고려를 많이 한다고 합니다. 다른 방법으로 Linear Decay, Inverse Sqrt(Inverse Square Root) Decay 도 있습니다.\n",
        "\n",
        "아래의 예제는 HuggingFace transformer 모듈에서 get_cosine_schedule_with_warmup라는 scheduler를 불러온 것인데, AdamW라는 Optimizer를 활용하고 있습니다. Epoch는 3으로 설정했고, 총 학습 단계 수는, 특정한 batch로 나뉘어진 데이터셋의 batch 수와 epoch를 곱한 만큼이 됩니다. Scheduler에 이를 활용하여 총 학습 단계 수(num_training_steps)를 정의해줍니다. num_warmup_steps는 활용하지 않아 0으로 설정이 되어있습니다. 이렇게 해서 정해진 학습률(lr)이, 매 학습 단계마다 제어되며, Step Decay를 활용한 학습률 Scheduling을 적용하게 되는 것입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "8iqDwv9fn3N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZHWY1tMomyQ",
        "outputId": "bf95641c-4ac0-43ee-ed1a-56733f4c4b1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scheduler PyTorch코드 예제\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "from transformers import AdamW\n",
        "# 옵티마이저 설정\n",
        "optimizer = AdamW(model.parameters(),\n",
        "lr = 1e-5, # 학습률\n",
        "eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값,\n",
        ")\n",
        "# 에폭수\n",
        "epochs = 3\n",
        "# 총 훈련 스텝\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Learning rate decay를 위한 스케줄러\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps=5, base_lr=0.3, final_lr=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "m4AmDwb9lbOO",
        "outputId": "08ded5aa-2e8a-47f0-e804-ab8204479269"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-eeb4ddd29af9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 옵티마이저 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m optimizer = AdamW(model.parameters(),\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 학습률\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-8\u001b[0m \u001b[0;31m# 0으로 나누는 것을 방지하기 위한 epsilon 값,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결국, 가중치를 규제(regularization)하는 방식과 비슷하게, 학습률을 규제하는(Learning Rate Decay)것이 Learning Rate Scheduler라고 할 수 있습니다.\n",
        "\n",
        "PyTorch에서는 기본적으로 다양한 learning rate scheduler를 제공하고 있습니다.\n",
        "\n",
        "## learning rate scheduler의 사용\n",
        "optimizer와 scheduler를 먼저 정의한 후, 학습할 때 batch마다 optimizer.step() 하고 epoch마다 scheduler.step()을 해주면 됩니다. 대략적인 코드를 작성하면 아래와 같은 코드의 진행입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "gDF5SIFnoxgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuRkAhvKpUjy",
        "outputId": "da13cae6-a792-4e0e-ed80-71d0380a3593"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting data\n",
            "  Downloading data-0.4.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from data) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from data) (4.4.2)\n",
            "Collecting funcsigs\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: data\n",
            "  Building wheel for data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7244 sha256=7b11b79c42ac1479f3f31a0b96355930d439cbb40e005ea3167979ea6e99b2d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/e8/fa/e253c256048ea58d99a8abb5e751abb6a838af6f12887b5418\n",
            "Successfully built data\n",
            "Installing collected packages: funcsigs, data\n",
            "Successfully installed data-0.4 funcsigs-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from data import AudioDataset, AudioDataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = nn.Linear(10, 10)\n",
        "        self.activation = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.linear1(x))\n",
        "\n",
        "# data\n",
        "tr_dataset = AudioDatset('tr')\n",
        "data_loader = AudioDataLoader(tr_dataset, batch_size=3, shuffle=1)\n",
        "# model\n",
        "model = Model()\n",
        "# loss\n",
        "loss = nn.MSELoss()\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "#scheduler\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
        "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
        "                                        last_epoch=-1,\n",
        "                                        verbose=False)\n",
        "\n",
        "epochs=100\n",
        "for epoch in range(epochs):\n",
        "    for i, (data) in enumerate(data_loader):\n",
        "        x_data, y_data = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        estimated_y = model(x_data)\n",
        "        loss = loss(y_data, estimated_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step() # you can set it like this!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "qVGS3VJbokhF",
        "outputId": "f203099e-4f57-43ee-e1e2-0b76cb55bde1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-92f3dab3ffac>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAudioDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AudioDataset' from 'data' (/usr/local/lib/python3.10/dist-packages/data/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LambdaLR\n",
        "Lambda 표현식으로 작성한 함수를 통해 learning rate를 조절한다. 초기 learning rate에 lambda함수에서 나온 값을 곱해줘서 learning rate를 계산한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "KVzkiJDKpiJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
        "                                        lr_lambda=lambda epoch: 0.95 ** epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "AJ0aKHWZpSF7",
        "outputId": "29be2918-1697-4030-f220-783f45751509"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e4c3b1d49283>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n\u001b[1;32m      3\u001b[0m                                         lr_lambda=lambda epoch: 0.95 ** epoch)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* optimizer: 이전에 정의한 optimizer 변수명을 넣어준다.\n",
        "* lr_lambda: lr에 곱해질 factor를 정하는 함수\n"
      ],
      "metadata": {
        "id": "FgybZ5cnppAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiplicativeLR\n",
        "Lambda 표현식으로 작성한 함수를 통해 learning rate를 조절한다. 초기 learning rate에 lambda함수에서 나온 값을 누적곱해서 learning rate를 계산한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "lM-AdWXnp4Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer=optimizer,\n",
        "                                                lr_lambda=lambda epoch: 0.95 ** epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "s8Z7OpajpkOU",
        "outputId": "4053e923-93cd-4988-b6e8-95798b784c08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5022e26cb9b0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer=optimizer,\n\u001b[1;32m      3\u001b[0m                                                 lr_lambda=lambda epoch: 0.95 ** epoch)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* optimizer: 이전에 정의한 optimizer 변수명을 넣어준다.\n",
        "* lr_lambda: lr에 곱해질 factor를 정하는 함수\n"
      ],
      "metadata": {
        "id": "AfRVbnPKp8aO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StepLR\n",
        "step size마다 gamma 비율로 lr을 감소시킨다. (step_size 마다 gamma를 곱한다.)\n",
        "\n"
      ],
      "metadata": {
        "id": "RZ8UvGfJp-_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "9oc-gSvNp7Ca",
        "outputId": "67d98138-a972-4428-ebfa-dab7f2528566"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-13e19cac6d2e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* optimizer: 이전에 정의한 optimizer 변수명을 넣어준다.\n",
        "* step_size: 몇 epoch마다 lr을 감소시킬지가 step_size를 의미한다.\n",
        "* gamma: gamma 비율로 lr을 감소시킨다.\n"
      ],
      "metadata": {
        "id": "fzVM_fenqLdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiStepLR\n",
        "step size가 아니라 learning rate를 감소시킬 epoch을 지정해줌\n",
        "\n"
      ],
      "metadata": {
        "id": "gkw1aginqPn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "3s0j8H8xqKG4",
        "outputId": "8e45ef11-14e1-42fc-f2fa-b4c5c5f88e54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2b7ea2de6999>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmilestones\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* optimizer: 이전에 정의한 optimizer 변수명을 넣어준다.\n",
        "* milestones: learning rate 줄일 epoch index의 list\n",
        "*gamma: gamma 비율로 lr을 감소시킨다.\n"
      ],
      "metadata": {
        "id": "lUjtgS-WqTEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReduceLROnPlateau\n",
        "성능이 향상이 없을 때 learning rate를 감소시킨다. 그렇기 때문에 validation loss나 metric(평가지표)을 learning rate step함수의 input으로 넣어주어야 한다. 그래서 metric이 향상되지 않을 때, patience횟수(epoch)만큼 참고 그 이후에는 learning rate를 줄인다. optimizer에 momentum을 설정해야 사용할 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "430yYa_mqWRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "for epoch in range(100):\n",
        "     train(...)\n",
        "     val_loss = validate(...)\n",
        "\n",
        "     # Note that step should be called after validate()\n",
        "     scheduler.step(val_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "qVOo1XTyqRoz",
        "outputId": "91a06166-97f8-48d4-e01b-5eafc85f4ae2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5c3a92d859d4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* optimizer: 이전에 정의한 optimizer 변수명을 넣어준다.\n",
        "* mode: 'min'이나 'max'중 하나의 모드로 설정한다. min은 metric이 감소를 멈출 때/ max는 metric이 증가를 멈출 때\n",
        "* factor: 감소시킬 비율 lr*factor default:0.1\n",
        "* patience: metric이 향상 안될 때, 몇 epoch을 참을 것인가?\n",
        "* threshold: 새로운 optimum이 될 수 있는 threshold (얼마 차이나면 optimum update되었다고 볼 수 있나?)\n",
        "* threshold_mode: dynamic threshold를 설정할 수 있다. 'rel' 이나 'abs' 중 하나의 모드로 설정한다. 'rel'모드이면 min일 때, best(1-threshold) max일 때, best(1+threshold)/ 'abs'모드이면 best+threshold\n",
        "* cool_down: lr이 감소한 후 몇 epoch동안 lr scheduler동작을 쉴지\n",
        "* min_lr: 최소 lr\n",
        "* eps: 줄이기 전, 줄인 후 lr의 차이가 eps보다 작으면 무시한다."
      ],
      "metadata": {
        "id": "ldq84_zeqcjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_cosine_schedule_with_warmup\n",
        "0과 옵티마이저에 설정된 초기 lr 사이에서 선형적으로 증가하는 워밍업 기간 후에 옵티마이저에서 0으로 설정된 초기 lr 사이의 코사인 함수 값에 따라 감소하는 학습률로 스케줄을 생성합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "rbKMJQQ4qutB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* optimizer (Optimizer) — The optimizer for which to schedule the learning rate.\n",
        "* num_warmup_steps (int) — The number of steps for the warmup phase.\n",
        "* num_training_steps (int) — The total number of training steps.\n",
        "* num_cycles (float, optional, defaults to 0.5) — The number of waves in the cosine schedule (the - defaults is to just decrease from the max value to 0 following a half-cosine).\n",
        "* last_epoch (int, optional, defaults to -1) — The index of the last epoch when resuming training.\n"
      ],
      "metadata": {
        "id": "W-lcOlMaqxdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_samples = 968\n",
        "bs = 32\n",
        "n_epochs = 10\n",
        "\n",
        "num_warmup_steps = (total_samples // bs) * 2\n",
        "num_total_steps = (total_samples // bs) * n_epochs\n",
        "\n",
        "model = nn.Linear(2, 1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n",
        "                                                         num_warmup_steps=num_warmup_steps, \n",
        "                                                         num_training_steps=num_total_steps)\n",
        "lrs = []\n",
        "for i in range(num_total_steps):\n",
        "    optimizer.step()\n",
        "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "TbQtZjzfqaq4",
        "outputId": "9a74010f-d003-4af7-a21a-127e92037e62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1385d2112b2b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n\u001b[0m\u001b[1;32m     11\u001b[0m                                                          \u001b[0mnum_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_warmup_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                          num_training_steps=num_total_steps)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference : PyTorch Optimizer https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "Guide to Pytorch Learning Rate Scheduling Guide to Pytorch Learning Rate Scheduling\n",
        "\n",
        "https://sanghyu.tistory.com/113 https://sanghyu.tistory.com/113\n",
        "\n"
      ],
      "metadata": {
        "id": "Ry3QF8JHq3H2"
      }
    }
  ]
}